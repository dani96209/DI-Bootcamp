{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JoxsebFGuk3",
        "outputId": "ed4f0a50-7a92-48bf-edd5-8c014abf04bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size required per group : 2941\n"
          ]
        }
      ],
      "source": [
        "#Exercise 1\n",
        "\n",
        "from statsmodels.stats.power import NormalIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "\n",
        "# Setting the parameters\n",
        "alpha = 0.05  # Level of significance\n",
        "power = 0.8   # Power of the test\n",
        "p1 = 0.20     # Open rate actual (variant Control)\n",
        "p2 = 0.23     # Open rate expected (variant Test)\n",
        "\n",
        "# Calculation of the Cohen effect (h)\n",
        "effect_size = proportion_effectsize(p1, p2)\n",
        "\n",
        "# Calculation of sample size by group\n",
        "sample_size = NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
        "\n",
        "# Displaying the result\n",
        "print(f\"Sample size required per group : {round(sample_size)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exercise 2\n",
        "from statsmodels.stats.power import NormalIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "\n",
        "# Setting the parameters\n",
        "alpha = 0.05  # Level of significance\n",
        "power = 0.8   # Power of the test\n",
        "p1 = 0.20     # Open rate actual (variant Control)\n",
        "p2 = 0.23     # Open rate expected (variant Test)\n",
        "\n",
        "# List of effect sizes (Cohen's h)\n",
        "effect_sizes = [0.2, 0.4, 0.5]\n",
        "\n",
        "# Calculating the sample size for each effect\n",
        "sample_sizes = {}\n",
        "for effect_size in effect_sizes:\n",
        "    sample_size = NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
        "    sample_sizes[effect_size] = round(sample_size)\n",
        "\n",
        "# Displaying results\n",
        "for effect_size, size in sample_sizes.items():\n",
        "    print(f\"Sample size needed for an effect of {effect_size}: {size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOYvtBEEH6uM",
        "outputId": "a4587ff2-903a-4ced-d662-bf87ec5cd1a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size needed for an effect of 0.2: 392\n",
            "Sample size needed for an effect of 0.4: 98\n",
            "Sample size needed for an effect of 0.5: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3\n",
        "\n",
        "from statsmodels.stats.power import NormalIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "\n",
        "# Setting the parameters\n",
        "alpha = 0.05  # Level of significance\n",
        "effect_size = 0.2  # Effect size (Cohen's h)\n",
        "power_levels = [0.7, 0.8, 0.9]  # Desired power levels\n",
        "\n",
        "# Calculating the sample size for each power level\n",
        "sample_sizes = {}\n",
        "for power in power_levels:\n",
        "    sample_size = NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
        "    sample_sizes[power] = round(sample_size)\n",
        "\n",
        "# Displaying results\n",
        "for power, size in sample_sizes.items():\n",
        "    print(f\"Sample size needed for a power of {power}: {size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heKUYO7yIaJl",
        "outputId": "ff515f1f-055e-4f92-f9ce-c46011cb64bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size needed for a power of 0.7: 309\n",
            "Sample size needed for a power of 0.8: 392\n",
            "Sample size needed for a power of 0.9: 525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4:\n",
        "\n",
        "Stopping Criteria for A/B Test:\n",
        "\n",
        "To define stopping criteria for your A/B test, we'll need to consider the following factors:\n",
        "\n",
        "Statistical Significance: A commonly used threshold for statistical significance is a p-value of less than 0.05. This means that if the p-value for Version B is less than 0.05, we can stop the test early because the difference between the versions is unlikely to be due to random chance.\n",
        "\n",
        "Minimum Detectable Effect (MDE): You should have a clear MDE before starting the test. This is the smallest effect size we are interested in detecting, based on our business goals. If one version achieves the MDE early, it can be a reason to stop the test.\n",
        "\n",
        "Sample Size Consideration: Even if the p-value reaches significance early, ensure that the sample size for both versions is sufficiently large to make the result reliable. Stopping too early with a small sample size can lead to false positives.\n",
        "\n",
        "False Positive Protection: Since we're using sequential testing (monitoring results as we go), we may want to account for the risk of Type I error (false positives). One common method is to use sequential testing correction methods such as the Bonferroni correction  to adjust the significance threshold over time.\n",
        "\n",
        " Use methods like false positive rate (FDR) control to adjust the significance threshold when multiple tests are performed over time. For example, after multiple tests, FDR  will help  avoid falsely reporting a significant result simply because of the increase in tests performed.\n",
        "\n",
        "Implementing Sequential Testing:\n",
        "\n",
        "Sequential testing allows us to monitor results during the test and potentially stop early if one version shows clear improvement. Here's how we can implement it:\n",
        "\n",
        "Predefined Monitoring Intervals: In your case, we plan to monitor results weekly. Each week, we analyze the p-value of the test.\n",
        "\n",
        "Stopping Rule: we can stop the test early if:\n",
        "\n",
        "The p-value is below your threshold (typically 0.05).\n",
        "\n",
        "The observed effect is large enough to warrant stopping based on our MDE.\n",
        "\n",
        "Adjust for Multiple Looks: When conducting sequential testing, each \"look\" at the data increases the risk of a false positive. To address this, we could apply a correction (e.g., Bonferroni or similar methods) or use a group sequential design to adjust the significance threshold based on the number of interim looks.\n",
        "\n",
        "Plan for Stopping: If at any of our monitoring intervals the p-value is below 0.05 and the sample size is sufficiently large, we stop the test. If it's above 0.05, we continue the test and wait for the next interval to review the results.\n",
        "\n",
        "At the End of Week Three:\n",
        "Version B p-value = 0.02: This means that after three weeks, Version B shows a statistically significant improvement (p-value < 0.05).\n",
        "\n",
        "What would we do next?\n",
        "\n",
        "Check sample size: Ensure that we have enough data points for both versions. A p-value of 0.02 is statistically significant, but if the sample size is too small, the result might not be reliable.\n",
        "\n",
        "Review MDE: If the improvement seen in Version B is greater than or equal to Minimum Detectable Effect (MDE), and the sample size is large enough, we can confidently stop the test early.\n",
        "\n",
        "Stop the Test: If the p-value is sufficiently low, the sample size is adequate, and the effect size meets your business goals, we can stop the test early and implement Version B as the winning variation.\n",
        "\n",
        "Consider Adjustments for Multiple Looks: If we’ve been checking the p-value multiple times, consider applying a correction (e.g., Bonferroni or other methods) to ensure that the result is not a false positive due to repeated testing.\n",
        "\n",
        "In summary, at the end of Week 3, if Version B’s p-value is 0.02, we could stop the test early if the sample size is large enough, the effect size is meaningful, and the result is statistically significant after accounting for multiple looks. If any of these conditions are not met, continue the test and check the results again in the coming weeks."
      ],
      "metadata": {
        "id": "ho1N_K5fHjkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5:\n",
        "\n",
        "Setting Up the Prior Belief:\n",
        "\n",
        "In a Bayesian approach, we begin by defining our prior belief about the new feature before any data is collected. This prior belief represents our initial assumption based on experience, intuition, or previous knowledge. For this scenario, since we believe the new feature has a 50% chance of improving user engagement, we can set our prior distribution as follows:\n",
        "\n",
        "Prior distribution: We can model our prior belief as a Beta distribution (a common choice for Bayesian analysis when dealing with probabilities), where the two parameters α and β are set to reflect your initial belief.\n",
        "\n",
        "Since we believe there's a 50% chance that the feature is better, we can set a Beta(1,1) distribution, which is a uniform distribution and reflects a neutral prior, indicating that all outcomes are equally likely at the start (a 50% chance of success).\n",
        "\n",
        "Alternatively, if we have additional confidence or knowledge (based on previous data or expert opinion), we might adjust this to something like Beta(2,2) to reflect more certainty that the feature has a moderate chance of success.\n",
        "\n",
        "Collecting Data and Updating the Belief (Posterior Distribution):\n",
        "\n",
        "Once we collect data, we update our belief about the probability that the new feature improves user engagement using Bayes' theorem. Bayes' theorem combines the prior distribution (your initial belief) with the likelihood of observing the data given a certain outcome, leading to an updated posterior distribution.\n",
        "\n",
        "Updated belief (Posterior): The data we collect will update the prior belief. For example, after analyzing the data, we find that the posterior probability of the new feature being better is 65%. This means that, after considering the evidence (the data), we are now 65% confident that the feature improves user engagement, given our initial belief and the data we've collected.\n",
        "\n",
        "Influence on Decision Making:\n",
        "\n",
        "The posterior distribution directly influences our decision. In this case, a 65% probability suggests that there is moderate evidence in favor of the new feature being better. Depending on our decision-making criteria, we might interpret the result as follows:\n",
        "\n",
        "If we have a threshold for decision making (e.g., 60% or 70% probability), a posterior probability of 65% might be high enough to justify adopting the new feature.\n",
        "\n",
        "If the new feature’s improvement is large enough to justify the cost or effort of implementation, then the 65% probability could be considered significant enough to make the decision to proceed.\n",
        "\n",
        "What if the Posterior Probability is Only 55%?\n",
        "\n",
        "If the posterior probability is only 55%, this would indicate that the data is weakly in favor of the new feature, but the evidence is not strong enough to make a confident decision.\n",
        "\n",
        "Decision-making options:\n",
        "\n",
        "Wait for more data: If we don’t have enough evidence (like in this case, with only a slight improvement over 50%), we might decide to collect more data and continue the experiment for a longer period or gather more diverse samples to get a clearer signal.\n",
        "\n",
        "Threshold-based decision: If we have a predetermined threshold for making decisions (e.g., 60% or higher), then 55% might be too low, and we may choose to not adopt the new feature at this stage, since the evidence is inconclusive.\n",
        "\n",
        "Review cost-benefit analysis: Even with a 55% posterior probability, we may decide to adopt the feature if the cost of further testing is high, or if we are willing to take a relatively small risk of failure for a potentially large reward.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Prior belief: Start with a neutral or informed prior, like Beta(1,1) or Beta(2,2).\n",
        "\n",
        "Posterior belief: After collecting data, we update our belief (e.g., 65% posterior probability) using Bayesian inference.\n",
        "\n",
        "Decision: If the posterior probability is 65%, we may decide to move forward with the feature. If the posterior probability is only 55%, we may choose to wait for more data or reconsider whether the feature is worth implementing."
      ],
      "metadata": {
        "id": "9T6fzzlXIJ52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6:\n",
        "\n",
        "1. Adjusting Traffic Allocation After the First Week:\n",
        "\n",
        "Since Layout C is showing higher engagement after the first week, we might want to increase the traffic allocated to Layout C in order to gather more data and potentially achieve a more definitive conclusion.\n",
        "\n",
        "Increase the Traffic to Layout C: To leverage the promising performance of Layout C, we could adjust the traffic allocation to give it more visitors. For example, we could change the traffic distribution to:\n",
        "\n",
        "Layout A: 25% of the traffic.\n",
        "\n",
        "Layout B: 25% of the traffic.\n",
        "\n",
        "Layout C: 50% of the traffic.\n",
        "\n",
        "This adjustment is done under the assumption that Layout C is performing better, and we want to gather more evidence to confirm it.\n",
        "\n",
        "Reason for Change: The adjustment would allow we to maximize the information gained from the better-performing layout, thus accelerating our decision-making process. We can also monitor the performance of Layout C more closely to ensure that it continues to show improvement and that the initial result wasn't just due to random variation.\n",
        "\n",
        "2. Adapting the Experiment in Subsequent Weeks:\n",
        "\n",
        "We can continue adapting the experiment over the next weeks based on the ongoing performance of the layouts. Here’s how to approach this:\n",
        "\n",
        "Continual Monitoring: Weekly or daily, monitor the engagement rates for all three layouts. If Layout C continues to show higher engagement, consider further increasing its share of the traffic.\n",
        "\n",
        "Stopping Criteria: Set up a stopping rule to determine when we have sufficient evidence to declare a winner. For instance, if Layout C shows statistically significant improvement over the other layouts, we can stop the experiment early and select it as the winner.\n",
        "\n",
        "Further Adjustments: As the experiment progresses, we might consider dynamic traffic allocation:\n",
        "\n",
        "If Layout C's performance plateaus or worsens in the coming weeks, we may decide to re-allocate some of its traffic back to the other layouts (A and B).\n",
        "\n",
        "Alternatively, if Layout C continues to outperform the others, we could increase its traffic allocation even further, say to 60-70%.\n",
        "\n",
        "3. Challenges with Adaptive Experimentation and How to Address Them:\n",
        "\n",
        "While adaptive experimentation (like adjusting traffic allocation based on early results) offers advantages, it can also present challenges. Here are a few potential issues and how to address them:\n",
        "\n",
        "a. Risk of False Positive (Selection Bias):\n",
        "\n",
        "Challenge: Early adaptations might lead us to prematurely favor a layout based on random fluctuations or insufficient data, resulting in a false positive (incorrectly declaring one layout as better).\n",
        "\n",
        "Solution: To minimize this, apply statistical corrections like Sequential Testing or Bayesian methods.\n",
        "\n",
        " For example:\n",
        "\n",
        "Sequential Testing: This involves adjusting the test to account for the fact that we are continually monitoring results, so we do not increase the risk of Type I errors (false positives) by checking the data too often.\n",
        "\n",
        "Bayesian Approach: This approach allows us  to continuously update your belief about which layout is best, taking into account prior data and new evidence.\n",
        "\n",
        "b. Compromising Statistical Power:\n",
        "\n",
        "Challenge: By adjusting traffic allocation dynamically, we may risk underpowering certain layouts (especially if we reduce their traffic too quickly), which could make it harder to detect meaningful differences between layouts.\n",
        "\n",
        "Solution: Ensure that the total sample size for each layout is large enough to maintain statistical power. We can calculate the minimum sample size needed upfront for each layout, and try to ensure that traffic allocation adjustments don’t lead to each layout being tested with too few users.\n",
        "\n",
        "c. Multiple Testing and False Discovery Rate:\n",
        "\n",
        "Challenge: When you adjust traffic dynamically and test multiple layouts, we're essentially running multiple tests at once. This increases the risk of false discoveries (incorrectly concluding that a layout is better just due to random chance).\n",
        "\n",
        "Solution: Use techniques like FDR (False Discovery Rate) correction or p-value adjustments to account for multiple tests. This ensures we don’t end up with spurious results just because we're testing multiple layouts simultaneously.\n",
        "\n",
        "d. Overfitting:\n",
        "\n",
        "Challenge: Adaptive experimentation might make us overly reliant on early data, leading to overfitting the experiment to the data we have, potentially ignoring future changes or trends.\n",
        "\n",
        "Solution: Continue to collect data for a reasonable duration and ensure that we base decisions on stable trends, not just short-term fluctuations. It may also help to regularly update our model or approach as new data comes in."
      ],
      "metadata": {
        "id": "uErwbDZkK939"
      }
    }
  ]
}